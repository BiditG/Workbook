{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Machine Learning\n",
    "### Artificial Intelligence 1, Week 7\n",
    "\n",
    "\n",
    "### Learning models for **classification** or **regression** from a set of labelled instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This week\n",
    "Learning outcomes:\n",
    "\n",
    "- Identify formulate and apply the basic processes of supervised machine learning\n",
    "- Understand the role of data in estimating accuracy \n",
    "\n",
    "Videos:\n",
    "- Basic model building process: train and test \n",
    "- Types of model: instance-based ( e.g. kNN) vs explicit (e.g. decision trees,rules, ...) \n",
    "- Example:   greedy rule induction as compared to expert system\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Paradigm\n",
    "- Completely different paradigm to symbolic AI\n",
    "- Create a system with the ability to learn\n",
    "- Present the system with series of examples\n",
    "- System builds up its own model of the world\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figures/PersonThinkingAboutDogs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/idealisedDog.png\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Video (6:52): Hello World of Machine Learning Recipes\n",
    "\n",
    "\n",
    "https://youtu.be/cKxRvEZd3Mw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It's all about the data\n",
    "- Computers cannot experience artefacts of the real world directly\n",
    "- Instead they just deal with a few variables that represent them\n",
    "- ML algorithms learn from a “training set” containing digital representations of examples to learn from\n",
    "- Outcomes depend entirely on:\n",
    " - What you choose to measure\n",
    " - And how representative your training set is\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More formally\n",
    "\n",
    "We have a set of *n* examples., and for each one  we have: \n",
    "- a value for each of *f* features \n",
    "- a label\n",
    "\n",
    "The data set *X* is usually 2-D array of *n* rows and *f* columns.   \n",
    "The label set *y* is usually a 1-D array with *n* entries.   \n",
    "For now we'll assume the features are *continous* (e.g. floating point values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the label comes from a discrete unordered set of *m* values, e.g.  (\"Orange\",\"Apple\" \"Banana\"): \n",
    "- we have a **Classification** problem.  \n",
    "- We learn a model *M* that is a mapping from a *f*-dimensional continuous space (the feature values) onto a finite set\n",
    "- *M*: R<sup>f</sup> --> \\{1,...,m\\}\n",
    "\n",
    "If the label is an ordinal value (integer,    floating point):\n",
    "- we have a **Regression** problem.\n",
    "- *M*:R<sup>f</sup>->R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The  Supervised Learning Workflow\n",
    "\n",
    "<img src=\"figures/ML_workflow.png\" style= \"float:right\">\n",
    "\n",
    "This diagram assumes you are trying out more than one type of algorithm or choice of parameter settings\n",
    "\n",
    "If you are just trying one algorithm you can skip the validation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Iris flowers <img src=\"figures/Iris-image.png\" style=\"float:right\">\n",
    "- Classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-species for iris flowers\n",
    "- three class problem:\n",
    " - so for some types of algorithm have to decide whether to make  \n",
    "   a 3-way classifier or nested 1-vs-rest classifers\n",
    "- most ML classifiers can get over 90%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import  week7_utils as W7utils\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "irisX,irisy = sklearn.datasets.load_iris(return_X_y=True)\n",
    "iris_features= (\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "iris_names= ['setosa','versicolor','virginica']\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "W7utils.show_scatterplot_matrix(irisX,irisy,iris_features,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap so far\n",
    "Machine Learning is about learning patterns from data. In supervised ML this means:\n",
    "\n",
    "**Training Data**: set of labelled examples, each characterised by values for *f* features  \n",
    "**X**: data - usually a 2D array with one row per example, one column for each feature  \n",
    "  (even images can be 'flattened' into this format).   \n",
    "**y** : the labels/target \n",
    "\n",
    "A supervised Machine Learning **Algorithm**\n",
    "\n",
    "A **performance criteria**: used to drive training and then estimate quality of model.  \n",
    "Depending on the **context** this might be accuracy,  precision, recall,...\n",
    "\n",
    "\n",
    "A **test set** to estimate the performance of the model on unseen data.  \n",
    "If this is not available separately, have to take out some data from the training set\n",
    " - crude way; single 70:30 train:test split, making sure you preserve the proportions of different classes\n",
    " - better way: split data into ten\n",
    "   - repeatedly train on 9/10 test on remaining 1/10, \n",
    "   - \"headline\" result is mean, but keep split results for statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important Idea!  Decision Surfaces\n",
    "<img src=\"figures/decisionRegions.png\" style=\"float:right\" width=40%>\n",
    "\n",
    "Each feature defines a dimension in *feature space*.\n",
    "\n",
    "Each example has specific values for each feature\n",
    "- so it occupies one point in feature space\n",
    "\n",
    "The aim of our model is to let us predict labels for any item\n",
    "- so it puts decision boundaries into that space to divide it into regions\n",
    "\n",
    "Symbolic Reasoning: \n",
    "- boundaries defined by our 'knowledge' \n",
    "- so can plot without needing data!\n",
    "\n",
    "Machine Learning: \n",
    "- use the training data to **estimate** where the boundaries should be\n",
    "- then plots model's prediction for lots of points over a grid  \n",
    "  to find the decision surface and boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Algorithms\n",
    "Typically a ML method consists of:\n",
    "\n",
    "1: A  representation for the decision boundaries\n",
    " - Each different arrangement of boundaries defines a unique model\n",
    " - Each unique model is defined by the set of values for variables specifying where the boundaries are.\n",
    " - Different types of models will have different variables.\n",
    " \n",
    "2: A learning algorithm to deciding how to change those variable values to move between models\n",
    " - last week we saw how the KMeans clustering algorirthm uses \"local search with random restarts\"\n",
    "\n",
    "ML Algorithms build models in different ways\n",
    "- but they don’t care what it is they are grouping\n",
    "- and it is **meaningless** to say they “understand”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some example ML methods\n",
    "The field of ML is fast growing and contains many complex methods and representations\n",
    "In this module I will just focus on a few simple ideas to give you a feel for what is out there.  \n",
    "- Instance-based learning (k-Nearest Neighbours) - this week\n",
    "- Decision trees and rule induction algorithms- this week\n",
    "- Artificial Neural Networks - weeks 7 and 8\n",
    "\n",
    "Next year: \n",
    "- Artificial Intelligence 2:  15 credits, semester 1 (AI and \"General\" pathways)\n",
    "and in particular\n",
    "- Machine Learning: 15 credits, semester 2     ( AI pathway)\n",
    "\n",
    "will cover more algorithms in greater depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instance-based Methods: Nearest Neighbour Methods\n",
    "- Do not explicitly represent class boundaries  \n",
    "  Construct them “on-the-fly” when queried\n",
    "- Store the set of training examples  \n",
    "  More efficient methods may not store all points\n",
    "- Use a metric to calculate distance between two points  \n",
    "  e.g. Euclidean (continuous), Hamming (binary), ...\n",
    "\n",
    "<img src=\"figures/kNN-steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Nearest Neighbour Classification \n",
    "<img src=\"figures/voronoi.png\" style=\"float:right\" width = 400 title=\"https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor\">\n",
    "\n",
    "**init(neighbours=k, distance metric =d)**  :  \n",
    "Specify k and a distance metric d(i,j) \n",
    "\n",
    "**fit(trainingData)** :  \n",
    "Store a local copy of the training data as two arrays:  \n",
    "model_X of shape (numTrainingItems , numFeatures),  \n",
    "model_y of shape( numTrainingItems)\n",
    "  \n",
    "**predict(newItems)** :  \n",
    "*Step 1:   Make 2D array **distances** of shape (num_newItems , numTrainingItems)*   \n",
    "FOREACH COMBINATION of newItem i  and trainingitem j  \n",
    "...SET **distances [i] [j]** = d (i,j) \n",
    "\n",
    "*Step 2: Make 2D array **votes** of shape(num_newItems, k)*  \n",
    "FOREACH newItem i  \n",
    "...Find the *k* columns of the row **distances[i]** with the smallest values  \n",
    "...Put the corresponding *k* labels from model_y into **votes[i]**  \n",
    "\n",
    "*Step 3: Store majority vote in a  1D array y_pred of size (numToPredict)*   \n",
    "FOREACH  newItem i  \n",
    "...SET y_pred[i] = most_common_value(votes[i]) \n",
    "\n",
    "RETURN y_pred\n",
    "\n",
    "Image adapted from Vornoi tesselation for kNN from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "class simple_1NN:\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.numTrainingItems = X.shape[0]\n",
    "        self.numFeatures = X.shape[1]\n",
    "        self.model_X = X\n",
    "        self.model_y = y\n",
    "        \n",
    "    def predict(self,newItems):\n",
    "        numToPredict = newItems.shape[0]\n",
    "        yPred = np.zeros((numToPredict),dtype=int)\n",
    "        \n",
    "        # measure distances - creates an array with numToPredict rows and num_trainItems columns\n",
    "        dist = np.zeros((numToPredict,self.numTrainingItems))\n",
    "        for new_item in range(numToPredict):\n",
    "            for stored_example in range(self.numTrainingItems):\n",
    "                dist[new_item][stored_example]= W7utils.euclidean_distance(newItems[new_item],self.model_X[stored_example ])\n",
    "\n",
    "        #make predictions: This is K=1, TO DO- in your own time extend to work with K>1\n",
    "        for item in range(numToPredict):\n",
    "            closest = np.argmin(dist, axis=1) \n",
    "            yPred[item] = self.model_y [ closest[item]]\n",
    "        \n",
    "        return yPred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iris\n",
    "\n",
    "We'll use a function from sklearn to do our train/test split here.\n",
    "\n",
    "This is handy because it shuffles the data and has options to make sure that we keep the same proportion of different classes in our training and testing data.\n",
    "\n",
    "\n",
    "            \n",
    "           \n",
    "We'll also make a **confusion matrix** to examine the predictions it makes\n",
    "rows = target labels,  columns = predicted labels\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# make train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "\n",
    "myKNNmodel = simple_1NN()\n",
    "myKNNmodel.fit(X_train,y_train)\n",
    "y_pred = myKNNmodel.predict(X_test)\n",
    "print(y_pred.T) #.t turns column to row so it sghows onscreen better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## how good are these results?\n",
    "We can use a neat numpy trick to find out if the predictions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print ( (y_test==y_pred))\n",
    "accuracy = 100* ( y_test == y_pred).sum() / y_test.shape[0]\n",
    "print(f\"Overall Accuracy = {accuracy} %\")\n",
    "\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(y_pred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n",
    "#and here's sklearn's built-in method\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred,display_labels= iris_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualising 1-NN if we just learn from the petal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "petals = X_train[:,2:4]\n",
    "myKNNmodel.fit(petals,y_train)\n",
    "y_pred = myKNNmodel.predict(X_test[:,2:4])\n",
    "accuracy = 100* ( y_test == y_pred).sum() / y_test.shape[0]\n",
    "print(f\"Overall Accuracy in 2D = {accuracy} %\")\n",
    "W7utils.PlotDecisionSurface(petals,y_train,myKNNmodel, \"1-Nearest Neighbour on petal features\", iris_features[2:4],minZero=True,stepSize= 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rule Induction Algorithms\n",
    "\n",
    "In Topic One we looked at 'Knowledge-Based systems'  \n",
    "where **humans provided the rules** for a situation.\n",
    "<img src=\"figures/rule-representation.png\" style=\"float:right\" width=50%>\n",
    "\n",
    "\n",
    "In supervised learning we are interested in how we can make   \n",
    "**machines learn the rules** for an application.   \n",
    "- e.g. **if** feature_n > threshold **then** prediction.\n",
    "\n",
    "To do that we need to have:\n",
    "1. A representation for rules  \n",
    "2. A way of assigning \"goodness\" to (sets of) rules.\n",
    "3. A way of algorithmically generating possible rules  \n",
    "   We have fixed sets of features,operators,outputs,  \n",
    "   We can **discretize** the thresholds for each feature    \n",
    "   So we can use nested loops to create all possible rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Greedy rule induction: keep choosing the next best rule\n",
    "- Typically exploit this in a greedy constructive hill climbing approach:  \n",
    "    Repeatedly generate all the rules we could add to existing set of rules (model),   \n",
    "    Then select and adding the one that discriminates most of the remaining unclassified data \n",
    "\n",
    "- Most existing algorithms tend to use rules built up of lots of axis-perpendicular decisions.   \n",
    "  For example the (useless) rule  *If( petal_length > 0.3) THEN (\"Setosa\")*   \n",
    "  Draws a line through feature space, at right angles to the petal_length axis, crossing it at 0.3.  \n",
    "  Puts the label \"setosa\" on one side, nothing on the other\n",
    "\n",
    "- As more rules are added, the model effectively builds labelled (hyper) boxes in space.  \n",
    "  Rest of space is given with the default (majority) label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pseudocode\n",
    "Model holds a set of rules and a score.  \n",
    "Score() uses ruleset in candidate solution to make predictions on training set  \n",
    " and sets model.score to -1 if any errors,  else number of correct predictions\n",
    " \n",
    "**Note that a set of rules may not cover every training example**\n",
    "\n",
    "    #step 0.\n",
    "    Preprocess (trainingset)  \n",
    "    SET currentModel with empty ruleset, score = 0  \n",
    "    \n",
    "    #Main loop- repeat until model contains a rule that predicts for ech item\n",
    "    WHILE (currentModel.score<trainingsetSize) DO  \n",
    "        \n",
    "        SET bestchild = emptyModel\n",
    "        FOR newRule in  (all_possible_rules)  # 4 nested for-loops\n",
    "            SET newModel = COPY(currentModel)\n",
    "            SET newModel = ADDRULE (newModel, newRule)\n",
    "            SET score = SCORE(newModel)\n",
    "            IF (newModel.score > bestChild.score)\n",
    "               SET bestChild= COPY(newModel)\n",
    "         IF (bestChild.score > currentModel.score)\n",
    "            SET currentModel=COPY (bestChild)\n",
    "    RETURN currentModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flow chart for model learning\n",
    "<img src=\"figures/rule-induction-flowchart-fit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flowchart for predicting with model\n",
    "<img src=\"figures/rule-induction-flowchart-predict.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees \n",
    "Tree-based structure can capture rules and more.\n",
    "\n",
    "Basic idea: divide input space using a set of axis-parallel lines by **\"growing\"** a tree\n",
    "\n",
    "1. Start with single node that predicts majority class label.\n",
    "2. Recursively:\n",
    " 1. measure the \"data purity\"  or \"information content\"  of the data that arrives at that node\n",
    " 2. examine each way of splitting data  you could put into that node, and measure the information content of the left and right child nodes you would get from the split\n",
    " 4. if the  \"best\" split is above some threshold then add it and repeat\n",
    " \n",
    "**This criteria for adding nodes is different to the rule induction algorithm, and gives you different trees**\n",
    "\n",
    "**Interior nodes** are equivalent to conditions in a rule  \n",
    "**Leaf Nodes** are the outputs: \n",
    " - class labels (classification tree), or \n",
    " - equation for predicting values (regression tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees for our example datasets\n",
    "using code from sklearn \n",
    "`class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)`\n",
    "\n",
    "Like all sklearn models it implements a fit() and predict() method\n",
    "\n",
    "Note the default criteria for splitting is the 'gini' indes = there are many available, this is a popular one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
    "fig.suptitle(\"Illustration of how Decision Trees select and insert nodes to increase data purity\")\n",
    "for depth in range (1,4):\n",
    "    DTmodel = DecisionTreeClassifier(random_state=1234, max_depth=depth,min_samples_split=2,min_samples_leaf=1)\n",
    "    DTmodel.fit(X_train,y_train)\n",
    "    _ = tree.plot_tree(DTmodel, feature_names=iris_features, class_names= iris_names,filled=True,ax=ax[depth-1])\n",
    "    ax[depth-1].set_title(\"Depth \"+str(depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualising the results using just the petal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "petals = X_train[:,2:4]\n",
    "Two_D_DecisionTree = DecisionTreeClassifier(max_depth=4)\n",
    "Two_D_DecisionTree.fit(petals,y_train)\n",
    "_ = tree.plot_tree(Two_D_DecisionTree, feature_names=iris_features, class_names= iris_names,filled=True)\n",
    "\n",
    "W7utils.PlotDecisionSurface(petals,y_train,Two_D_DecisionTree, \"Decision Tree: simplified outcomes\", iris_features[2:4],stepSize=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So how do  we learn models?\n",
    "**Construction**:  add boundaries to make models more complex\n",
    "- Add examples to kNN\n",
    "- Repeatedly add nodes to trees, splitting on new variables\n",
    "- Repeatedly add rules that classify as-yet unclassified data\n",
    " - Add nodes to an artifical neural network\n",
    " \n",
    "**Perturbation**: Move existing boundaries to change model\n",
    "- Change value of K or distance function in kNN\n",
    "- Change rule/treenode thresholds: *if  exam < 40*  &rarr; *if exam < 38*\n",
    "- Change operators in rules/ tree nodes:  *if exam < 38* &rarr; *if exam &leq; 38*\n",
    "- Change variables considered in rules/tree nodes: *if exam < 38* &rarr; *if coursework < 38*\n",
    "- Change weights in MLP, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "Supervised Machine Learning is concerned with learning predictive models from datasets\n",
    "- Different algorithms use different representations of decision boundaries\n",
    "- Regions inside the boundaries contain **Class labels** or **(formulas leading to) continuous values** (regression)\n",
    "\n",
    "Algorithms **fit** models to data by repeatedly:\n",
    "  - making and testing small changes,  \n",
    "  - and then selecting the ones that improve accuracy on the training set\n",
    "  - until some stop criteria is met\n",
    "\n",
    "  - They do this by either adding complexity or changing the parameters of an existing model\n",
    "  - This is equivalent to moving through “model space”\n",
    "\n",
    "Once the model has been learned (fit) we leave it unchanged  \n",
    "  - and use it to **predict** the labels for new data points\n",
    "\n",
    "Next week:   Neural Networks\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
