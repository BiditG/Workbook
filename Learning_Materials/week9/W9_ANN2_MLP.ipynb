{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#    Multi Layer Perceptrons\n",
    "## Artificial Intelligence 1, week 9 \n",
    "\n",
    "This week:\n",
    "- recap on perceptrons\n",
    "- perceptrons are linear classifiers\n",
    "- multi-layer perceptrons\n",
    "   - architecture\n",
    "   - feed forward predictions\n",
    "   - back propagation for training\n",
    "   - examples\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neurons - The basis of  Neural Networks\n",
    "\n",
    "<div class=\"pull-right\"><img src=\"figures/neuron.jpg\"></div>\n",
    "Perceptrons, invented by Frank Rosenblatt in the late 1950's,\n",
    "are a form of supervised machine learning algorithm inspired by neuron cells.\n",
    "\n",
    "In neurons, signals come in along the dendrites and out along the axon. \n",
    "\n",
    "A synapse is the connection between the axon of one cell and the dendrites of another.\n",
    "\n",
    "Crudely, input signals are 'summed' and if they reach a certain threshold the neuron 'fires'\n",
    "and sends a signal down the synapse to the\n",
    "connected cells.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptrons - The basis of Artificial Neural Networks\n",
    "\n",
    "<div class=\"pull-right\"><img src=\"figures/Perceptron.png\", width = 90%/></div>\n",
    "Perceptrons are an algorithmic approximation of this process and can learn to solve simple classification problems.\n",
    "\n",
    "Input values are multiplied by a learnable parameter called a *weight*.  \n",
    "If the sum of the inputs $\\times$ weights is over a certain threshold (0), \n",
    "the Perceptron 'fires' and generates an output.\n",
    "\n",
    "To let us adapt the threshold in the same way as the weights we provide a *bias* signal.\n",
    "\n",
    "We use the *error* in the output to change the value of the *weights* by a small amount - the *learning rate*.  \n",
    "The process is repeated until the error is 0, or as small as we can get it.\n",
    "\n",
    "**Note:** The threshold which determines if the Perceptron produces an output is determined by its *activation function*.  \n",
    "For Perceptrons this is often a step function which outputs a 1 or 0 i.e. 'fires' or not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptrons Create Linear Decision Boundaries\n",
    "\n",
    "<img src=\"figures/straightLine.png\" style=\"float:right\" width=300>\n",
    "\n",
    "\n",
    "### Perceptron's output changes when the sum of the weighted  inputs is 0  \n",
    "So if we plot the output at different points in space there is a  *decision boundary*.\n",
    "\n",
    "This happens when  $input1 \\times weight1 + input2 \\times weight2 + biasweight = 0$\n",
    "\n",
    "We can rearrange this equation to see that the change of behaviour happens when   \n",
    "$input2 = - \\frac{weight1}{ weight2} \\times input1  - \\frac{ biasweight}{ weight2}$\n",
    "\n",
    "But this is just the equation for a straight line !\n",
    "- slope (m) is given by -( weight1/ weight2)\n",
    "- the intercept, c  = -( biasweight / weight2)\n",
    "\n",
    "- step function => which side of the line is 1 or 0\n",
    "\n",
    "So, the Perceptron is essentially learning a function for a straight line which is called the decision boundary.\n",
    "In this case, which 'class' the set of inputs belongs to i.e. True or False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "import W9_utils as utils\n",
    "%matplotlib inline\n",
    "\n",
    "weight1 = widgets.FloatSlider(value=-0.5,min = -1,max = 1)\n",
    "weight2 = widgets.FloatSlider(value=0.5,min = -1,max = 1)\n",
    "biasweight = widgets.FloatSlider(value=-0.5,min = -1,max = 1)\n",
    "funcToModel = widgets.RadioButtons(options=['OR','AND','XOR'])\n",
    "output=interact(utils.showPerceptron, w1=weight1,w2=weight2,bias=biasweight,func = funcToModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Training Law\n",
    "Every time you present an example you compare the output to the desired value,  \n",
    "then update each weight in turn using:\n",
    "\n",
    "    ∆ω_i = ε · i_i · α\n",
    "\n",
    "    change in weight_i (for this example)\n",
    "            = error (for this example)\n",
    "              X input_i (for this example)\n",
    "              X learning rate (fixed)\n",
    "\n",
    "So this means that \n",
    "- Error = target-actual, can be negative \n",
    "- Weights only change when there is an error \n",
    "- Only active inputs are changed. \n",
    "- Inactive (x=0) inputs are not changed at all (which makes sense since they did not contribute to the error). \n",
    "\n",
    "See AI illuminated p297+ for a worked example\n",
    "\n",
    "### In the next cell we will update the simple perceptron we created in week 7 to have an init and a  fit() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class two_input_perceptron:\n",
    "    def  __init__( self,learningRate):\n",
    "        self.weight1 = random()\n",
    "        self.weight2 = random()\n",
    "        self.biasweight = random()\n",
    "        self.learningRate = learningRate\n",
    "        print(\" starting with initial random weights {:.4f}, {:.4f} and {:.4f}\".format(self.weight1,self.weight2,self.biasweight))\n",
    "        \n",
    "    def predict(self, input1, input2) -> int:\n",
    "        summedInput = input1*self.weight1 +input2*self.weight2 + 1*self.biasweight #bias is always +1\n",
    "        if summedInput>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def update_weights( self, in1, in2, target):\n",
    "        error = target - self.predict(in1,in2)\n",
    "        if(error == 0):\n",
    "            return 0\n",
    "        else:\n",
    "            self.biasweight += error * 1   * self.learningRate # bias is always +1\n",
    "            self.weight1    += error * in1 * self.learningRate\n",
    "            self.weight2    += error * in2 * self.learningRate           \n",
    "            return 1\n",
    "                \n",
    "    def fit(self,train_X,train_y, maxEpochs,verbose=True):\n",
    "        for epoch in range (maxEpochs):\n",
    "            errors = 0\n",
    "            for testcase in range (len(train_y)):\n",
    "                errors += self.update_weights(train_X[testcase][0], train_X[testcase][1],train_y[testcase])\n",
    "            if(errors >0):\n",
    "                if( epoch < (maxEpochs-1) and verbose):\n",
    "                    print(\" in epoch {} there were {} errors\".format(epoch,errors))\n",
    "                elif(epoch==maxEpochs-1):\n",
    "                    print(\" after {} epochs there were still {} errors\".format(epoch,errors))\n",
    "            else:\n",
    "                print(\" Perceptron solved the learning problem in {} epochs\".format(epoch))\n",
    "                break\n",
    "    def getWeights(self):\n",
    "        return self.biasweight, self.weight1, self.weight2\n",
    "                \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_X = np.asarray( [[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "and_y = [0,0,0,1]\n",
    "or_y = [0,1,1,1]\n",
    "xor_y = [0,1,1,0]\n",
    "\n",
    "print('Learning AND')\n",
    "myPerceptron = two_input_perceptron(0.1)\n",
    "myPerceptron.fit(train_X,and_y, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print('\\nLearning OR')\n",
    "myPerceptron3 = two_input_perceptron(0.1)\n",
    "myPerceptron3.fit(train_X,or_y, 20,verbose=False)\n",
    "\n",
    "\n",
    "print('Learning XOR')\n",
    "myPerceptron2 = two_input_perceptron(0.1)\n",
    "myPerceptron2.fit(train_X,xor_y, 1000,verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptrons - Training and Testing on Real Data \n",
    "### we'll use a virginica: not virginica for the  Iris data\n",
    "\n",
    "Truth table data and logical functions are a good way to learn the Perceptron algorithm but the data isn't very realistic.\n",
    "\n",
    "Most problems are much more complex and cannot be represented with binary data or solved with only 4 training examples.  \n",
    "We were also only training for one **step** (one input example) or one **epoch** (all input examples) at a time, so that we\n",
    "could see what the algorithm was doing.\n",
    "\n",
    "In supervised learning, generally we want to stop training either:\n",
    "- when there is no improvement in the number of errors on the training data\n",
    "- or after some fixed number of epochs\n",
    "\n",
    "Once training is finished we apply the model (trained weights) to some test data and\n",
    "measure its performance.  \n",
    "This gives us an indication of how well it would perform on new data it has not 'seen' before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "\n",
    "feature_names = ['sepal width','sepal_length','petal_width','petal_length']\n",
    "irisLabels = np.array(('setosa','versicolor','virginica'))\n",
    "#turn it into a 2-class sproblems setos:nonsetosa\n",
    "iris_twoclass_y = np.where(irisy==1,0, irisy)\n",
    "iris_twoclass_y = np.where(irisy==2,1, iris_twoclass_y)\n",
    "\n",
    "\n",
    "iris_train_X, iris_test_X, iris_train_y, iris_test_y = train_test_split(irisX,iris_twoclass_y, test_size=0.33, stratify=iris_twoclass_y, random_state=5 )\n",
    "\n",
    "train_x= iris_train_X[:,1:3]\n",
    "test_x = iris_test_X[:,1:3]\n",
    "train_y = iris_train_y\n",
    "test_y = iris_test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the training data\n",
    "figure, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "ax[0].scatter(x=train_x[:,0], y=train_x[:,1], c=train_y,cmap='Set1')\n",
    "ax[0].title.set_text(\"Train\")\n",
    "ax[1].scatter(x=test_x[:,0], y=test_x[:,1], c=test_y,cmap='Set1')\n",
    "ax[1].title.set_text(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "myPerceptron = two_input_perceptron(0.05)\n",
    "myPerceptron.fit(train_x,train_y, 50,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bias_w,w1,w2= myPerceptron.getWeights()\n",
    "print(\"Trained model weights:w1: \" + str(w1) + \" w2: \" + str(w2) + \" bw: \" + str(bias_w))\n",
    "\n",
    "# Create the range of values for decision boundary\n",
    "x_range = np.linspace(train_x[:,0].min(), train_x[:,0].max(), 10)\n",
    "y_range = [((-w1/w2) * x) + (-bias_w/w2) for x in x_range]\n",
    "# Plot the training and test data and the decision boundary\n",
    "figure, ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "ax[0].scatter(train_x[:,0], train_x[:,1], c=train_y,cmap='Set1')\n",
    "ax[0].set_title(\"Training Data\")\n",
    "ax[0].plot(x_range, y_range, color='r')\n",
    "ax[1].scatter(test_x[:,0],test_x[:,1],c=test_y,cmap='Set1')\n",
    "ax[1].set_title(\"Test Data\")\n",
    "_=ax[1].plot(x_range, y_range, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Do you understand why the test set accuracy is a better estimate than the training set accuracy?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Going beyond single straight decision boundaries\n",
    "<div style=\"float: right; width: 50%\">\n",
    "<img align=\"right\", src=\"figures/mlp.png\", width = 75%/,height=40%/>\n",
    "    </div>Some problems need more than one single decision boundary, or  curved (non-linear) boundaries.\n",
    "\n",
    "So we need a more flexible architecture made out of the same building blocks.\n",
    "\n",
    "Multi-Layer Perceptrons (MLPs) have:\n",
    "* lots of connected perceptrons with trainable weights arranged in layers.\n",
    "* calculations flow layer-by-layer from inputs to outputs\n",
    "\n",
    "At the output layers we know the targets and the computed activations \n",
    "* so we can use the perceptron training rule to adjust the last set of weights\n",
    "* But we need some adjustment to know how to adjust the weights from inputs to hidden layer nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Changing the activation function to tell us what the actual input signal was\n",
    "<div style=\"float: right; width: 50%\"><img src=\"figures/sigmoid.png\", width = 80%/></div>\n",
    "Instead of using a step function which loses all the detail, we use something with one-to-one mapping\n",
    "\n",
    "Several options, most common is the sigmoid $\\sigma(x)$\n",
    "\n",
    "$ output = \\sigma(input)$\n",
    "\n",
    " $ \\;\\;\\;\\;\\;\\;= \\frac{1}{ 1+e^{-input}}$\n",
    " \n",
    " Which has the nice property that it's derivative (useful for working backwards) is $\\sigma(x) (1- \\sigma(x))$\n",
    "\n",
    "Nowadays people also use **relu** activation: $relu(x) = MAX(0,x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Back-propagation of errors according to their causes \n",
    "Since we don’t know what the ‘expected’ output of the hidden layer is, we cannot calculate the error.\n",
    "\n",
    "Instead, we share out the error from an output neurone to each hidden neurones.\n",
    "\n",
    "We do this in proportion to the strength of the signal coming from that hidden neurone. \n",
    "\n",
    "In practice we can feed in lots of samples then take the average of their errors\n",
    "\n",
    "### Back-propagations is now commonly called \"Stochastic Gradient Descent\" to reflect the fact that it is just a local search algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLP Predicting output for a sample with activation 1/(1+input)\n",
    "<img style=\"float:right\" width=50% src=\"figures/mlp-prediction.png\">\n",
    "\n",
    "**Input to node h1** = $w_1x_1 + w_3x_2$\n",
    "\n",
    "**Output from node h1** = $\\sigma(w_1x_1 +w_3x_2)$\n",
    "\n",
    "**Input to node h2** = $w_2x_1 + w_4x_2$\n",
    "\n",
    "**Output from node h2** = $ \\sigma(w_2x_1 + w_4x_2)$\n",
    "\n",
    "**Input to output node O** = $ w5\\ \\sigma(w_1x_1 + w_2x_2)  + w6\\ \\sigma(w_2x_1 + w_4x_2)$\n",
    "\n",
    "**Output O** =   $ \\sigma( \\ w5\\ \\sigma(w_1x_1 + w_2x_2)  + w6\\ \\sigma(w_2x_1 + w_4x_2) ) $\n",
    "\n",
    " \n",
    "In production code  we can represent the signals (inputs, each layer of hidden nodes, output nodes) as  vectors and the weights between each layer as a matrix\n",
    "\n",
    "Then we can do the whole thing as a sequence of vector-matrix multiplications: **which is exactly what GPUs are designed to do extremely efficiently!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLP: error back propagation for weight updates with a single sample     <img style=\"float:right\" width=40% src=\"figures/mlp-errors.png\">\n",
    "\n",
    "Step 1. Updates weights to final layer using 'real' diference between target and output\n",
    "- change in w5 = error * output from H1 * learning rate   \n",
    "  = $E1 * \\sigma(w_1x_1 +w_2x_2) * \\alpha$  \n",
    "  \n",
    "- change in w6 = error * output from H2  * learning rate.  \n",
    "  = $ E1 *  \\sigma(w_2x_1 + w_4x_2) * \\alpha$\n",
    "  \n",
    "\n",
    "\n",
    "Step 2. Calculate share of error to feed back to hidden nodes\n",
    "-  E2 = (signal to Output from h1)/ (total signal input to Output)  \n",
    "  = $\\frac {\\sigma(w_1x_1 +w_2x_2) }{ \\sigma(w_1x_1 +w_2x_2) +\\sigma(w_2x_1 + w_4x_2 )}$  \n",
    "- similar for E3\n",
    "\n",
    "3. Use these to update w1, w3 like perceptron training\n",
    "\n",
    "4. Same process for rest of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What does this look like in code?\n",
    "\n",
    "As we mentioned above, it is 'cleanest' to implement this using vector and matrix algebra\n",
    "\n",
    "There are many toolkits that do this - prehaps the best known is Google's tensorflow\n",
    "\n",
    "Nowadays libraries like keras let us hide many of the details to provide a quick clean coding interface\n",
    "with *lots* of options\n",
    "\n",
    "For now we will use the sklearn implemntation:\n",
    "`class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)[source]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setting up the final layer of  our network for the type of labels \n",
    "- For a binary problem:\n",
    "    - we have *one* output node with sigmoid (logistic) activation,\n",
    "     - and  interpret its output as the probabilty that the item belongs to class 1\n",
    "    - usually threshold at 0.5 to make a prediction, but probabilities can be informative\n",
    "- For a  problem with M>2 classes:\n",
    "    - we usually have M output nodes\n",
    "    - 'onehot' encoding for y  \n",
    "       e.g. 0 1 0 0 instead of '2'\n",
    "    - 'softmax' ('all or nothing') activation function for final layer  \n",
    "      e.g. node ouptuts (0.2 0.5 0.4 0.6) get transformed to (0 0 0 1)\n",
    "- For regression problem:\n",
    "    - usually one output node with a linear activation  \n",
    "      e.g. output = summed weighted inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "xor_X = np.asarray( [[0,0],[0,1],[1,0],[1,1]])\n",
    "xor_y = [0,1,1,0]#XOR\n",
    "\n",
    "# one hidden layer with 5 neurons logistic (sigmoid) activation and Stochastic Gradient Descent (backprop)\n",
    "xorMLP = MLPClassifier( hidden_layer_sizes = (5),activation='logistic',solver='lbfgs')\n",
    "xorMLP.fit(xor_X,xor_y)\n",
    "\n",
    "accuracy = 100*  xorMLP.score(xor_X,xor_y)\n",
    "print('Estimated accuracy using the test set is {} %'.format(accuracy))\n",
    "utils.plotDecisionSurface(xorMLP,xor_X,xor_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# and for the blob data\n",
    "# one hidden layer with 5 neurons logistic (sigmoid) activation and Stochastic Gradient Descent (backprop)\n",
    "continuousMLP = MLPClassifier( hidden_layer_sizes = (10,),activation='logistic',solver='lbfgs')\n",
    "continuousMLP.fit(train_x,train_y)\n",
    "\n",
    "accuracy = 100*  continuousMLP.score(train_x,train_y)\n",
    "print('Estimated accuracy using the training set is {} %'.format(accuracy))\n",
    "utils.plotDecisionSurface(continuousMLP,train_x,train_y)\n",
    "\n",
    "print(f\" Estimated accuracy on test set is {100*  continuousMLP.score(test_x,test_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# And it's trivial to add more hidden nodes or add layers if we want to ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Layer Perceptrons  today\n",
    "\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/2/26/Deep_Learning.jpg\" style=\"float:right\" width = 400>\n",
    "\n",
    "Since the 1980s Artificial Neural Networks have been hugely successful,   \n",
    "across a range of classification, regression and control problems.\n",
    "\n",
    "Only problems with MLPs were:\n",
    "- reliance on creating appropriate features \n",
    "- reliance on  appropriate scaling\n",
    "- lack of interpretablility\n",
    "\n",
    "\n",
    "Deep Neural Networks:   MLP with >5 hidden layers\n",
    "\n",
    "Typically complex early layers to find/create features such as:\n",
    "- Convolutional Layers discover spatial patterns e.g. 1D,   2D (images) N-D (video)  \n",
    "  each node acts like a small loical pattern detecting filter that 'sweeps over' the image \n",
    "- Recurrent Layers discover patterns in time e.g. speech recognition, natural language processing  \n",
    "  each node has a memory and (often) a 'forget-gate' that responds to patterns  \n",
    "  e.g. speech marks `\"`in text or `/*` and `*/` for comments in code \n",
    "\n",
    "Because they have more weights to learn appropriate values for, Deep Networks need:\n",
    "- *lots* of data, \n",
    "- *lots* of computational power / time to train\n",
    "\n",
    "Deep Neural Networks useually have one or more  \"dense\" layers (like in a MLP) before the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A simple deep convolutional network for the MNIST data\n",
    "<img src=\"https://miro.medium.com/max/3744/1*SGPGG7oeSvVlV5sOSQ2iZw.png\" width = 1200>\n",
    "image from https://towardsdatascience.com/mnist-handwritten-digits-classification-using-a-convolutional-neural-network-cnn-af5fafbc35e9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "The perceptron training algorithm is guaranteed to converge on a solution – if one exists.  \n",
    " - But this will only be the case for linearly separable problems.\n",
    " - and if more than one decision voundary separates the data, there are no guarantees which you will get\n",
    "\n",
    "Multi-Layer Perceptrons solve this problem by combining perceptrons in a layered structure\n",
    "- The network is capable of learning making non-linear decision boundaries\n",
    "- Activation functions changed from a step, to something that preserves more information sigmoid (logistic)  \n",
    "  so you can make big[small] changes to weights leading to a hidden node if it's summed inputs were   lots [a little] over the 'trigger point'\n",
    "\n",
    "Backpropagation (Stochastic Gradient Descent)) is the algorithm used to train the more complex, multi layered networks\n",
    "- **signals propagate forwards** through the network.\n",
    "- **errors are propagated backwards** through the network. \n",
    "- For a given input, hidden nodes receive an error signal *in proportion to* the strength of their forwards signal\n",
    "- It is a form of local search so can get stuck on local optima \n",
    "\n",
    "Deep Neural Networks are MLP with extra layers to learn features from complex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "rise": {
   "enable_chalkboard": true,
   "footer": "<h3>Intro to AI 2019-20 Jim Smith, UWE</h3>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
