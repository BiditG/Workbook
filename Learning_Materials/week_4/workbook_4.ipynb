{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae6289d-998e-42ed-8ade-d9a55526d67b",
   "metadata": {},
   "source": [
    "# Workbook 4: Local Search in categorical and continuous spaces\n",
    "\n",
    "Overview of activities and objectives of this workbook:\n",
    "\n",
    "1. The first part of this workbook will implement Local Search for a simple *binary* problem (One-max).\n",
    "   - The One-max problem is simple; given an a random binary array, turn all values (i.e. 0's) to 1.\n",
    "   - We will also evaluate the performance of this algorithm on the One-max problem with different complexities (number of values).\n",
    "\n",
    "2. The second part of this workbook will adapt the binary One-max problem to use *continuous* decision variables.\n",
    "   - We will adapt the Local Search algorithm from part one to solve the continuous One-max problem.\n",
    "   - For local search with continuous variables we will explore two different methods for generating the neighbouring candidate solutions; adding random (gaussian) noise, and using gradient information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ac125",
   "metadata": {},
   "source": [
    "# Part 1: Local Search for binary One-max\n",
    "\n",
    "Unlike the search algorithms covered in previous weeks, Local Search only considers the immediate neighbours of the current best candidate solution. If any of the neighbours are an improvement on the current candidate solution then that solution is selected and the remaining neighbours are ignored/forgotten i.e. removed from the open list (see pseudocode below).\n",
    "\n",
    "You can think of Local Search like climbing a hill. Each step you choose the direction that takes you higher towards the top and not sideways or backwards. And you aren't allowed to backtrack by considering steps you *could* have taken 2 or 3 steps ago.\n",
    "\n",
    "To consider local search in terms of the One-max problem:\n",
    "\n",
    "1. If the random starting solution is [0, 1, 1, 0, 1], with quality 3\n",
    "\n",
    "2. We then generate several neighbouring solutions, e.g.:\n",
    "    - [0, 1, 1, 0, 0] with quality 2\n",
    "    - [0, 1, 1, 1, 0] with quality 3\n",
    "    - [0, 1, 1, 1, 1] with quality 4\n",
    "\n",
    "3. Then choose the best neighbour generated. In this case the 3rd one improves the quality so that is selected. The other two have either worse, or the same quality and so are discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd62e37-11a7-4d2c-b1be-2928935831e5",
   "metadata": {},
   "source": [
    "### Pseudocode for Local Search SelectAndMoveFromOpenList function\n",
    "\n",
    "<div style=\"background:#F0FFFF;font-size:18pt\">\n",
    "<p style=\"color:darkred;font-size:18pt;margin-bottom:0pt\"><em>SelectAndMoveFromOpenList</em></p>\n",
    "<dl style=\"font-size:18pt;margin-top:0pt\">\n",
    "    <dt>&nbsp;&nbsp;&nbsp;<b>IF</b> IsEmpty( open_list) <b>THEN</b> </dt>\n",
    "    <dd> RETURN None</dd>\n",
    "    <dt> &nbsp;&nbsp;&nbsp;<b>ELSE</b></dt>\n",
    "    <dd>bestChild &larr; <b>GetMemberWithHighestQuality</b>(openList)</dd>\n",
    "    <dd> <b>EMPTY</b>(openlist)&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"background:pink\">This prevents backtracking</span></dd>\n",
    "    <dd>  <b>IF</b> BetterThan(bestChild, bestSoFar) <b>THEN</b> <br>\n",
    "        &nbsp;&nbsp;&nbsp;&nbsp;bestSoFar &larr; bestChild <br>\n",
    "        &nbsp;&nbsp;&nbsp;&nbsp;RETURN bestChild </dd>\n",
    "    <dd> <b>ELSE</b> <br>&nbsp;&nbsp;&nbsp;&nbsp; RETURN None</dd>\n",
    "</dl>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19af297-72bb-491a-996c-a0042b2d6f9b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2> Activity 1: Implementing local search for the binary One-max problem</h2>\n",
    "    <ol>\n",
    "    <li>Complete the second cell below which implements the <code>LocalSearch</code> class.</li>\n",
    "    <li>We have provided an <code>__init__()</code> method with over-rides the default behaviour and creates a random starting point</li>\n",
    "    <li>You need to complete the method <code>select_and_move_from_openlist()</code>. We have broken this down into <b>4</b> clearly marked small steps:</li>\n",
    "        <ul>\n",
    "            <li>Find the best candidate solution on the open list</li>\n",
    "            <li>Clear the open list</li>\n",
    "            <li>Check if the best candidate solution is a better solution</li>\n",
    "            <li>Return the best candidate solution if it is an improvement, else return <code>None</code></li>\n",
    "        </ul>\n",
    "    <li> Test your implementation by running the third cell which uses your implementation to solve the <em>oneMax</em> problem.\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651ff3e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Hints:</b>\n",
    "The first step (finding the best solution on the open list) is the same as <b>Best-first search</b> from the previous week. You can use <code>self.a_better_than_b()</code> to evaluate if one solutions quality is better than another.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ccbd6-c5e7-45de-a75f-483aae36409a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from candidatesolution import CandidateSolution\n",
    "from singlemembersearch import SingleMemberSearch\n",
    "from problem import Problem\n",
    "from onemaxproblem import OneMaxBinary, OneMaxContinuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3cea1-eb18-43ff-b1ac-5f5b8c46b2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalSearch(SingleMemberSearch):\n",
    "    \"\"\"Implementation of local search.\"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\" return name\"\"\"\n",
    "        return \"local search\"\n",
    "    \n",
    "    def __init__( self,\n",
    "        problem: Problem,\n",
    "        constructive: bool = False,\n",
    "        max_attempts: int = 50,\n",
    "        minimise=True,\n",
    "        target_quality=1):\n",
    "        \"\"\" call super class \n",
    "        then change to random starting point\n",
    "        \"\"\"\n",
    "        super().__init__(problem,\n",
    "                         constructive=constructive,\n",
    "                         max_attempts=max_attempts,\n",
    "                         minimise=minimise,\n",
    "                         target_quality=target_quality)\n",
    "        \n",
    "        # over-ride default\n",
    "        arrays_of_rands = np.random.choice(my_binary_onemax.value_set, size=num_vars)\n",
    "        start_point =  self.open_list[0]\n",
    "        start_point.variable_values= list(arrays_of_rands)\n",
    "\n",
    "        # measure quality \n",
    "        start_point.quality = self.problem.evaluate(start_point.variable_values)\n",
    "        if start_point.quality == self.target_quality:\n",
    "            self.trials = 1\n",
    "            self.result = start_point.variable_values\n",
    "            self.solved = True\n",
    "\n",
    "    def select_and_move_from_openlist(self) -> CandidateSolution:\n",
    "        \"\"\"Pops best thing from list, \n",
    "        clears rest of list, \n",
    "        then returns best thing\n",
    "        relies on the presence of self.best_so_far\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        next working candidate (solution) taken from open list\n",
    "           **if it is an improvement**\n",
    "        None\n",
    "           IF list is empty OR next thing is worse than best so far\n",
    "        \"\"\"\n",
    "        next_soln = CandidateSolution()\n",
    "\n",
    "        # check the open list isn't empty\n",
    "        if len(self.open_list) == 0:\n",
    "            self.runlog += \"LS:empty open list\\n\"\n",
    "            return None\n",
    "\n",
    "        # get best child: start looking for it in position 0\n",
    "        best_index = 0\n",
    "        quality = self.open_list[0].quality\n",
    "        best_so_far: int = quality\n",
    "        # ====> insert your code below to copy the best solution from the open list into next_soln\n",
    "        \n",
    "        # <==== insert your code above to copy the best solution from the open list into next_soln\n",
    "\n",
    "        self.runlog += (\n",
    "            f\"\\t best child quality {best_so_far},\"\n",
    "            f\"\\n\\t best so far {self.best_so_far}\\n\"\n",
    "        )\n",
    "        # clear the openlist\n",
    "        # ====> insert your code below here to clear the openlist\n",
    "        \n",
    "        # <==== insert your code above here to clear the openlist\n",
    "\n",
    "        # always accept first move\n",
    "        improvement_found: bool \n",
    "        if self.trials == 1:\n",
    "            improvement_found = True\n",
    "        # otherwise there must be an improvement\n",
    "        else:\n",
    "            pass\n",
    "            # value will depend on whether next_soln.quality improves on self.best_so_far\n",
    "            # ====> insert your code below to set the value of variable improvement_found after first trial\n",
    "        \n",
    "            # <==== insert your code above to set the value of variable improvement_found after first trial\n",
    "\n",
    "        \n",
    "        # return best offspring from open list or None if it doesn't improve on self.best_so_far\n",
    "        # ====> insert your code below to manage the return\n",
    "        \n",
    "        # <==== insert your code above manage the return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a8024-6964-4821-a605-494e24d122de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and create problem instance\n",
    "num_vars = 20\n",
    "my_binary_onemax = OneMaxBinary(N=num_vars)\n",
    "\n",
    "# create search\n",
    "my_search = LocalSearch(my_binary_onemax,\n",
    "                       constructive = False,\n",
    "                       max_attempts= 500,\n",
    "                       minimise=False,\n",
    "                       target_quality=num_vars)\n",
    "\n",
    "starting_quality = my_search.open_list[0].quality\n",
    "success = my_search.run_search()\n",
    "\n",
    "if success:\n",
    "    print(f'Run found the goal ({num_vars})'\n",
    "          f'starting from point with quality {starting_quality}'\n",
    "          f'after examining {my_search.trials} solutions.')\n",
    "else:\n",
    "    print(f'Run failed to solve the problem in {my_search.max_attempts} trials\\n'\n",
    "          f'runlog is:\\n {my_search.runlog}')\n",
    "    completed_ok=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff9ec1-c088-4958-a1a9-a6bd7c1ffb6a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2> Activity 2: Evaluating your implementation Local Search</h2>\n",
    "    Once your code works and the cell above runs and finds a solution, it is time to evaluate its performance.\n",
    "    Because it usually starts from a different random place every time, Local Search is a <b>stochastic</b> algorithm (the technical term for an algorithm that has a <b>random</b> element). This means that to analyse its behaviour we should run it several times and report the <em>average</em> number of solutions it tries before it finds the goal.<br>\n",
    "    <br><b>How to get started:</b>\n",
    "    <ol>\n",
    "    <li>For each of the problem sizes (10, 15, 20, 25, 30) (i.e. num_vars) we will run the search 10 times and record the number of attempts needed to solve the problem.</li>\n",
    "    <li>Then plot your results as a curve of mean values (y-axis) vs num_vars (x-axis) with error bars showing the standard deviation. The cell below shows you first introduction to the graphics package <b>matplotlib</b>.</li>\n",
    "    <li>To do this you can use two nested loops. See hints below.\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a5170",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Hints:</b>\n",
    "If you have three arrays for the problem size (x-axis), number of attempts for each size (y-axis), and standard deviations for each size, then you can make a nice plot using the code snippets provided below.<br>\n",
    "\n",
    "You can automate finding the values for these arrays with two nested loops.<br>\n",
    "\n",
    "The first loop is <code>for size in sizes</code> e.g. problem sizes (10, 15, 20, 25, 30):\n",
    "    <ul>\n",
    "        <li>Make an array called <code>attempts</code> full of zeros of size REPETITIONS (e.g. 10)</li>\n",
    "        <li>Then the second (inside) loop <code>for run in range(REPETITIONS)</code>:\n",
    "            <ul>\n",
    "                <li>make a new instance of the problem, of the appropriate size</li>\n",
    "                <li>make a new search object <code>my_search</code></li>\n",
    "                <li>call the <code>my_search.runsearch()</code> method</li>\n",
    "                <li>store the number of solutions it looked at <code>my_search.trials</code> in <code>attempts[run]</code></li>\n",
    "            </ul>\n",
    "    <li>Now you can use numpy's built in functions e.g.<code>np.mean(attempts)</code> and <code>np.std(attempts)</code> to calculate and store the mean and standard deviation of the number of attempts for this problem size</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3bddf-79e8-462e-9b0f-835b9fe8b280",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "<b>How to examine results when the algorithm contains randomness:</b><br>\n",
    "Lots of AI algorithms- both for search/optimisation and machine learning - use some form of randomness. This means that you might get a different result each time you run them on the same problem (or dataset). So two understand or compare results (scientists typically call these <i>observations</i>) we need to look at:\n",
    "<div>\n",
    "    <div style=\"float:right\">\n",
    "    <img src=\"https://curvebreakerstestprep.com/wp-content/uploads/2021/04/standard-deviation.png\" width=\"300\" height=\"300\">\n",
    "    </div>\n",
    "    <ol> \n",
    "        <li>The average case behaviour.<br>\n",
    "        Normally we use the <b>mean</b>, which is calculated as the sum of the observed values, divided by the number of observations.</li>\n",
    "        <li>The amount of difference between observations.<br>\n",
    "        Usually we use the <b>Standard Deviation</b>, a measure of how much, on average, results differ from the mean (ignoring the sign of the difference).</li>\n",
    "        </ol>\n",
    "</div>\n",
    "To give a simple example, lets say you run a test in which 5 people score 10, and 5 people score 0.<br>\n",
    "The mean= (5*10 + 5*0)/10 = 5, but the standard deviation = 5 as well - since everyone gets a score 5 different from the mean. If we rerun the test but this time everyone gets 4 or 6. Now our mean is still 5 (5*4 + 5*6 = 50), but the standard deviation will be 1. So smaller values of standard deviation means the results are more similar to each other.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b69ef3-8c20-42a5-8e48-a2fc4e090f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max attempts 10000\n",
    "MAX_ATTEMPTS = 10000\n",
    "\n",
    "# number of repetitions\n",
    "REPEATS = 10\n",
    "\n",
    "sizes = [10, 15, 20, 25, 30]\n",
    "means = np.zeros(len(sizes))\n",
    "std_deviations=np.zeros(len(sizes))\n",
    "\n",
    "# ====> insert your code below here\n",
    "\n",
    "# copy-paste the code from the cell above and wrap it in a loop \n",
    "# that stores the number of solutions tested in each run \n",
    "\n",
    "# after that loop report mean, and standard deviation of these\n",
    "\n",
    "# <==== insert your code above here\n",
    "\n",
    "# for making the plots\n",
    "from matplotlib import pyplot as plt\n",
    "# plot results    \n",
    "plt.errorbar(sizes, means, yerr=std_deviations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d167c5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:black;width:100%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48f64f",
   "metadata": {},
   "source": [
    "# Part 2: Local Search for continuous One-max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e1ed83-167e-45ed-a935-83243d0aca06",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2> Activity 3: Adapting local search for a continuous problem</h2>\n",
    "For continuous problems you will need to adapt your local search class. This requires adapting more of the methods from the single member search class.<br>\n",
    "The below code changes the <code>__init__</code> method to initialise with appropriate continuous values, and stores the number of samples to take from the neighbourhood each iteration (<code>sample_size</code>), and whether to use gradient-based search or not (<code>use_gradient</code>).<br>\n",
    "\n",
    "<b>Note:</b> this is a <em>Stretch</em> activity so don't worry if you can't complete it easily.\n",
    "\n",
    "<br><b>How to get started:</b>\n",
    "    <ol>\n",
    "        <li>Adapt the <code>select_and_move_from_openlist(self)</code> method from your <code>LocalSearch</code> class so that it now accepts solutions that are as good as <code>self.best_so_far</code> and not just improvements.</li>\n",
    "        <li>Over-ride the <code>run_search()</code> method so that it:\n",
    "            <ul> \n",
    "                <li>Generates a number of neighbours defined by <code>sample_size</code></li>\n",
    "                <li>For each neighbour, creates a set of changes (one for each variable) and adds those to the current values (then truncates to the valid range of values using <code>self.truncate_to_range</code>)\n",
    "                <li>IF <code>self.use_gradients</code> is <code>False</code> it generates the list of changes at random. ELSE it calls <code>self.problem.get_gradient()</code> then multiplies the result by <code>self.learning_rate</code> to get the changes</li>\n",
    "                <li>After looking at all of the neighbours, if they were all worse than what we had already, the open_list will be empty, so you  you need to put the <em>working_candidate</em> back on the open list instead of the closed list.</li>\n",
    "            </ul>\n",
    "    </ol> \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4c34b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Hints:</b>\n",
    "    <ol>\n",
    "        <li>The first step is very similar to the first activity. The key difference is we are now keeping candidate solutions that are equally as good as the current best.</li>\n",
    "        <li>For the second step, copy and paste the <code>run_search()</code> method from the <code>SingleMemberSearch</code> class into the class below. You will need to make your changes in the bit that <b>generates</b> and <b>tests</b> new candidate solutions.</li>\n",
    "        <li>Note: this version of the problem has a quality function that is the difference to the target so it needs to be minimised.</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e72ed-36f5-4bde-a15a-f3c053b44c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "   \n",
    "class LocalSearchContinuous(SingleMemberSearch):\n",
    "    \"\"\"Implementation of local search for continuous problems.\n",
    "      Assumes the search mode is perturbative.\n",
    "      Extends single member search by doing explicit sampling of neighbourhood\n",
    "      and if not stopping if no improvement is  found in an iteration\n",
    "\n",
    "      Parameters\n",
    "      ---------\n",
    "      sample_size(int): \n",
    "          number of neighbours to generate each iteration\n",
    "          default 10\n",
    "\n",
    "      use_gradient(bool): \n",
    "          whether to use the gradient instead of random changes\n",
    "          if the problem supports it.\n",
    "          If set, assume sample_size is 1\n",
    "          default False\n",
    "\n",
    "      learning_rate(float)\n",
    "          multiplier for gradient if used\n",
    "          default 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"local search continuous\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        problem: Problem,\n",
    "        constructive: bool = False,\n",
    "        max_attempts: int = 50,\n",
    "        minimise:bool=True,\n",
    "        target_quality:float=1,\n",
    "        sample_size:int = 10,\n",
    "        use_gradient:bool=False,\n",
    "        learning_rate=0.5):\n",
    "        super().__init__(problem, constructive=constructive,\n",
    "                       max_attempts=max_attempts,\n",
    "                       minimise=minimise,\n",
    "                       target_quality=target_quality)\n",
    "        \n",
    "        print(f'self.target_quality is {self.target_quality}')\n",
    "\n",
    "        # reinitialise to random continuous values in right range\n",
    "        self.num_vars  = len(self.open_list[0].variable_values)\n",
    "        for decision in range(self.num_vars):\n",
    "            self.open_list[0].variable_values[decision]= self.rand_in_range()\n",
    "\n",
    "        # re-evaluate\n",
    "        quality = self.problem.evaluate(self.open_list[0].variable_values)\n",
    "        self.open_list[0].quality=quality    \n",
    "\n",
    "        # store the number of neighbours to examine each iteration \n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        # does the problem support calculation of gradients\n",
    "        self.use_gradient = use_gradient\n",
    "        self.learning_rate = learning_rate\n",
    "        if self.use_gradient:\n",
    "            try:\n",
    "                _ = self.problem.get_gradient()\n",
    "                self.sample_size = 1\n",
    "            except:\n",
    "                self.use_gradient=False\n",
    "\n",
    "    def rand_in_range(self)->float:\n",
    "        \"\"\"generates a random number in the range\n",
    "        specified by the problem\"\"\"\n",
    "\n",
    "        lowest_val = self.problem.value_set[0]\n",
    "        val_range = self.problem.value_set[1] - self.problem.value_set[0]\n",
    "        return np.random.random()*val_range +lowest_val\n",
    "    \n",
    "    def get_rand_normals_in_range(self)->list:\n",
    "        \"\"\"generates random number form  normal distribution\n",
    "        mean= midpoint of valid range for problem\n",
    "        sdev = 10% of valid range. for problem\"\"\"\n",
    "\n",
    "        changes=[]\n",
    "        valrange = self.problem.value_set[1]-self.problem.value_set[0]\n",
    "        valmean =  (self.problem.value_set[1]+ self.problem.value_set[0])/2\n",
    "        for pos in range(self.num_vars):\n",
    "            randval= np.random.normal() *0.1*valrange + valmean\n",
    "            changes.append(randval)\n",
    "        return changes\n",
    "        \n",
    "    def truncate_to_range(self, val:float)->float:\n",
    "        \"\"\" truncates a val ot the valid range\n",
    "        defined by a problem\"\"\"\n",
    "\n",
    "        if val>self.problem.value_set[1]:\n",
    "            val = self.problem.value_set[1]\n",
    "        if val < self.problem.value_set[0]:\n",
    "            val = self.problem.value_set[0]\n",
    "        return val\n",
    "    \n",
    "    def select_and_move_from_openlist(self) -> CandidateSolution:\n",
    "        \"\"\"Pops best thing from list, clears rest of list, then returns best thing\n",
    "        relies on the presence of self.best_so_far\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        next working candidate (solution) taken from open list\n",
    "           *if it is an improvement*\n",
    "        None\n",
    "           IF list is empty OR next thing is worse than best so far\n",
    "        \"\"\"\n",
    "\n",
    "        next_soln = CandidateSolution()\n",
    "\n",
    "       # check the open list isn't empty\n",
    "        if len(self.open_list) == 0:\n",
    "            self.runlog += \"LS:empty open list\\n\"\n",
    "            return None\n",
    "\n",
    "        # get best child: start looking for it in position 0\n",
    "        best_index = 0\n",
    "        quality = self.open_list[0].quality\n",
    "        best_so_far: int = quality\n",
    "\n",
    "        # ====> insert your code below to copy the best solution from the open list into next_soln\n",
    "        \n",
    "        # <==== insert your code above to copy the best solution from the open list into next_soln\n",
    "        \n",
    "        self.runlog += (\n",
    "            f\"\\t best child quality {best_so_far},\\n\\t best so far {self.best_so_far}\\n\"\n",
    "        )\n",
    "        # clear the openlist\n",
    "        # ====> insert your code below here to clear the openlist\n",
    "        \n",
    "        # <==== insert your code above here to clear the openlist\n",
    "        \n",
    "        # always accept first move\n",
    "        improvement_found: bool \n",
    "        if self.trials == 1:\n",
    "            improvement_found = True\n",
    "        # otherwise must be an improvement or at least as good (to keep search going)\n",
    "        #i.e best_so_far must be at least as good as self.best_so_far\n",
    "        else:\n",
    "            pass\n",
    "            # ====> insert your code below to set the value of variable improvement_found after first trial\n",
    "        \n",
    "            # <==== insert your code above to set the value of variable improvement_found after first trial\n",
    "\n",
    "        # return best offspring from open list or None if it doesn't improve on self.best_so_far\n",
    "        # ====> insert your code below to manage the return\n",
    "        \n",
    "        # <==== insert your code above manage the return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eab913-601a-4048-820a-2acc354a01b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and create problem instance\n",
    "num_vars = 10\n",
    "continuous_onemax = OneMaxContinuous(N=num_vars)\n",
    "\n",
    "# search using option 1 from the lectures - adding gaussian noise\n",
    "my_search2 = LocalSearchContinuous(continuous_onemax,\n",
    "                        constructive = False,\n",
    "                        max_attempts= 500,\n",
    "                        minimise=True,\n",
    "                        target_quality=0.0)\n",
    "\n",
    "success = my_search2.run_search()\n",
    "if success:\n",
    "    print(f'Local Search solved the problem '\n",
    "          f'after {my_search2.trials} attempts.\\n'\n",
    "          f'solution {my_search2.result}\\n'\n",
    "          f'quality {my_search2.problem.evaluate(my_search2.result)[0]}')\n",
    "else:\n",
    "    print(f'failed to solve the problem in {my_search2.max_attempts} trials\\n'\n",
    "          f'runlog is:\\n {my_search2.runlog}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72077101-54e9-4089-97b5-4d1e4dc67e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search using option 2 from the lectures - using the gradient information\n",
    "mysearch3 = LocalSearchContinuous(continuous_onemax,\n",
    "                        constructive = False,\n",
    "                        max_attempts= 500,\n",
    "                        minimise=True,\n",
    "                        target_quality=0.0,\n",
    "                        use_gradient=True,\n",
    "                        learning_rate=0.5)    \n",
    "\n",
    "success = mysearch3.run_search()\n",
    "if success:\n",
    "    print(f'Local Search solved the problem '\n",
    "          f'after {mysearch3.trials} attempts.\\n'\n",
    "          f'solution {mysearch3.result}\\n'\n",
    "          f'quality {mysearch3.problem.evaluate(my_search2.result)[0]}')\n",
    "else:\n",
    "    print(f'failed to solve the problem in {mysearch3.max_attempts} trials\\n'\n",
    "          f'runlog is:\\n {mysearch3.runlog}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543dd5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Save and close Jupyter:</b>\n",
    "    <ol>\n",
    "        <li>Use the jupyterlab functions to download your work (ask your tutor if you need help with this) and save it somewhere sensible so you can find it easily.</li>\n",
    "        <li>Shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
