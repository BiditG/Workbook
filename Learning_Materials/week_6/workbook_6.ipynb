{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook 6: Supervised Machine Learning\n",
    "\n",
    "Overview of activities and objectives of this workbook:\n",
    "\n",
    "1. The first part of this workbook will introduce the K Nearest Neighbour algorithm for supervised learning.\n",
    "    - We will use the Iris dataset we introduced in the previous week.\n",
    "    - You are provided code for 1NN (1 nearest neighbour) and will extend this, using generative AI, to KNN (k nearest neighbours).\n",
    "    - We will also introduce several common Sklearn functions for splitting data into training and test sets, and visualising the results of classifiers.\n",
    "\n",
    "2. The second part of this workbook will introduce Decision Trees, another supervised learning algorithm.\n",
    "    - We will use the Sklearn implementation and explore how to limit tree growth (number/depth of branches) to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:black;width:100%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: K nearest neighbours (KNN) <img src=\"figures/Iris-image.png\" style=\"float:right\">\n",
    "\n",
    "First we will load the Iris data. This is a classic Machine Learning Data set which contains:\n",
    "- 4 measurements (features): sepal and petal width and length\n",
    "- 50 examples from each sub-species for iris flowers (so, 150 total)\n",
    "- 3 class labels: Iris-Virginica, Iris-Setosa or Iris-Versicolor\n",
    "\n",
    "The next cell to imports some useful libraries and then loads the iris dataset into two arrays:\n",
    "- <code>irisX</code> (the features - 150 rows x 4 columns)\n",
    "- <code>irisy</code> (the class labels - For the purpose of this tutorial we are going to ignore the fact that we are provided with class labels)\n",
    "- We'll also make a list of the <code>feature_names</code> so we can use them to label our plots.\n",
    "- Then we'll make a scatter plot to visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import week6_utils as w6utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris data\n",
    "iris_data = load_iris(return_X_y=False)\n",
    "# Extract the data and labels, feature names, and label names\n",
    "irisX = iris_data.data\n",
    "irisy = iris_data.target\n",
    "feature_names = iris_data.feature_names\n",
    "label_names = iris_data.target_names\n",
    "\n",
    "print(f\"Iris has {irisX.shape[0]} samples and {irisX.shape[1]} features: {feature_names}\")\n",
    "print(f\"Iris has 3 classes: {label_names}\")\n",
    "\n",
    "# Create a scatter plot of all the Iris data\n",
    "w6utils.show_scatterplot_matrix(irisX, irisy, feature_names, \"Iris Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing K-Nearest Neighbours\n",
    "\n",
    "Implementations of supervised ML algorithms typically have two functions `fit()` and `predict()`.\n",
    "\n",
    "**'Fitting'** -  means building/training the model with the training data, so that it (hopefully) makes good predictions.\n",
    "   - With KNN this just means storing the training data.\n",
    "   - But as we will see in future weeks this can also mean updating the model itself based on the quality of its predictions (e.g. with artificial neural networks).\n",
    "\n",
    "**Predicting** - means using the model to make predictions e.g. the class of a particular data example.\n",
    "\n",
    "For KNN predicting the label of a new example from the test set:\n",
    "1. Measure distance to example point from every member of the training set.\n",
    "2. Find the K Nearest Neighbours.  \n",
    "   - In other words, the K members of the training set with the smallest distances (*calculated in step 1*)\n",
    "3. Count the labels of those K training items and return the most common one as the predicted label.\n",
    "\n",
    "Below is a figure illustrating the start and first two steps of process.  \n",
    "It is followed by a code cell with a simple implementation of a class for 1-Nearest neighbours i.e. only consider the 1 closest neighbour. \n",
    "\n",
    "<b>Read through the code  to get a sense for how it implements the algorithm.</b><br>\n",
    "Your tutor will discuss it with you in the lab sessions.\n",
    "<img src=\"figures/kNN-steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "<b>Enumerating lists with python</b><br>\n",
    "Quite often we may want to go through every item in a list and know:\n",
    "<ul>\n",
    "    <li> What the item is</li>\n",
    "    <li> what position it is in (index)</li>\n",
    "</ul>\n",
    "We can do this in fewer lines of code by using python's built-in <code>enumerate</code> function.<br>\n",
    "For example, this code snippet:<br>\n",
    "<pre><code>my_list = ('a', 'b', 'c')\n",
    "for idx, name in enumerate(my_list):\n",
    "    print(idx , name)</code></pre>\n",
    "produces the output:<br>\n",
    "0 a<br>\n",
    "1 b<br>\n",
    "2 c<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Simple1NNClassifier:\n",
    "    \"\"\" Simple example class for 1-Nearest Neighbours algorithm.\n",
    "    Assumes numpy is imported as np and uses euclidean distance\n",
    "    \"\"\"    \n",
    "    def dist_a_b(self, a:np.array, b:np.array)->float:\n",
    "        \"\"\" euclidean distance between same-size vectors a and b\"\"\"\n",
    "        assert a.shape==b.shape, 'vectors not same size calculating distance'\n",
    "        return np.linalg.norm(a-b) \n",
    "    \n",
    "    def fit(self, x:np.ndarray, y:np.array):\n",
    "        \"\"\" just stores the data for k-nearest neighbour\"\"\"\n",
    "        self.model_x = x\n",
    "        self.model_y = y\n",
    "        self.is_fitted_=True\n",
    "        \n",
    "    def predict(self, new_items:np.ndarray):\n",
    "        \"\"\" makes predictions for an array of new items\"\"\"\n",
    "\n",
    "        # First item in  a numpy array's shape  is the number of rows\n",
    "        # Get the number of new items and the number of stored items\n",
    "        num_new = new_items.shape[0]\n",
    "        num_stored = self.model_x.shape[0]\n",
    "        \n",
    "        # Create empty array to store predictions\n",
    "        y_pred = np.zeros(num_new, dtype=int)\n",
    "        \n",
    "        # Measure distances - creates an array with one row for each new item and one column for each stored training example\n",
    "        distances = np.zeros((num_new, num_stored))\n",
    "        for row, new_item in enumerate(new_items):\n",
    "            for col, stored_example in enumerate(self.model_x):\n",
    "                distances[row][col]= self.dist_a_b(new_item, stored_example)\n",
    "\n",
    "        # Make predictions - for each new example, find the stored example with the smallest distance\n",
    "        for item_idx in range(num_new):\n",
    "            y_pred[item_idx] = self.predict_one(item_idx, distances)\n",
    "        \n",
    "        # Return the predictions\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_one(self, item_idx:int, distances:np.ndarray):\n",
    "        \"\"\" makes a class prediction for a single new item\n",
    "        This version is just for 1 Nearest Neighbour\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        item_idx: int\n",
    "            index of item to make prediction for - i.e. idx of row in distances matrix\n",
    "\n",
    "        distances: numpy ndarray\n",
    "            array of distances between new items (rows) and training set records (columns)\n",
    "        \"\"\"\n",
    "        # We're going to use numpy's argmin() method\n",
    "        # which gives us the get indexes of column with lowest value in an array\n",
    "        idx_of_nearest_neighbour = np.argmin(distances[item_idx])\n",
    "\n",
    "        # Return the predicted class of the nearest neighbour\n",
    "        return self.model_y[ idx_of_nearest_neighbour]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying 1NN to the Iris data\n",
    "\n",
    "We'll use the simple 1NN (K=1) classifier defined above and train and test on the Iris data we loaded previously.\n",
    "\n",
    "The next few cells demonstrate how to do this using some Sklearn functions/classes.\n",
    "\n",
    "### Split the data into train and test sets\n",
    "- `test_size` argument specifies how much of the data to keep back for testing (0.33 for the iris data is 50 for testing and 100 for training).\n",
    "- `stratify` argument makes sure our data has the same proportions of classes in train and test set (1:1:1 for the iris data set as this is *balanced*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make train/test split of datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy)\n",
    "\n",
    "print(f'Training set has {train_x.shape[0]} examples, test set has {test_x.shape[0]} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the model class then *fit*  it to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_1NN_model = Simple1NNClassifier()\n",
    "my_1NN_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model's performance on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions for test data\n",
    "predictions = my_1NN_model.predict(test_x)\n",
    "print(f'Predictions are:\\n {predictions}')\n",
    "\n",
    "# Make array of True/False values for each prediction\n",
    "# By comparing the predictions to the actual label values\n",
    "print(f'Prediction matches to actual label values are:\\n{test_y==predictions}')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"\\nOverall Accuracy = {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualise a confusion matrix\n",
    "A confusion matrix shows the counts of predictions vs the true label for each example. So correct predictions appear on the diagonal.\n",
    "\n",
    "This is often more useful than just calculating accuracy because it shows where the classifier is making mistakes.\n",
    "\n",
    "The Iris data is quite easy, so most models will make correct predictions for *setosa*  \n",
    "but misclassify one or two instances of *versicolor* and *virginica*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_predictions(test_y, predictions, display_labels=label_names)\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the decision boundary\n",
    "\n",
    "The decision boundary shows how, within the 'decision space' the model is making predictions.\n",
    "\n",
    "This can be useful to compare how different algorithms make decisions but is hard for most people to recognise in more than 2 dimensions.\n",
    "\n",
    "So we will quickly train a model using a 2D version of the iris data set (just the petal measurements).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Make data using numpy slicing to just pull the last two columns for every row\n",
    "petal_trainx = train_x[:, :2]\n",
    "petal_testx = test_x[:, :2]\n",
    "\n",
    "# Instantiate and fit model to data\n",
    "my_1NN_2d = KNeighborsClassifier(n_neighbors=1)\n",
    "my_1NN_2d.fit(petal_trainx, train_y)\n",
    "\n",
    "# Make predictions, score them \n",
    "y_pred = my_1NN_2d.predict(petal_testx)\n",
    "accuracy = 100 * ( test_y == y_pred).sum() / test_y.shape[0]\n",
    "num_errors= len(y_pred) - (test_y == y_pred).sum()\n",
    "print(f\"Overall Accuracy in 2D = {accuracy}%, model makes {num_errors} mistakes\")\n",
    "\n",
    "# Create the decision boundary learned by model from training data\n",
    "disp = DecisionBoundaryDisplay.from_estimator(my_1NN_2d, petal_trainx, alpha=0.5)\n",
    "\n",
    "# # Show where the test is within the decision boundary\n",
    "disp.ax_.scatter(petal_testx[:, 0], petal_testx[:, 1], c=test_y, edgecolor='black', label='test')\n",
    "_= disp.ax_.set_title(\"1NN decision boundary on petal features\")\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 1: Complete K-nearest neighbours with GenAI</h2>\n",
    "    The <code>Simple1NNClassifier()</code> class above only implements KNN for K=1, i.e. only considers the single closest neighbour when making predictions. In this activity you will extend (via inheritance) the <code>Simple1NNClassifier()</code> into a full KNN classifier. In other words, to make a predictions for a new example: finds the K nearest neighbours and assigns the label of the most common class amongst the neighbours.<br><br>\n",
    "    You need to complete the <code>SimpleKNNClassifier()</code> class below by:\n",
    "    <ol>\n",
    "        <li>Over-riding the <code>def __init__()</code> to take one parameter <code>K</code>, the number of neighbours to consider</li>\n",
    "        <li>Over-riding the <code>predict_one()</code> method so that:\n",
    "            <ol>\n",
    "                <li>Finds the indexes of the <code>self.K</code> nearest neighbours from the <code>distances</code> array.<li>Stores the labels of these neighbours.</li>\n",
    "                <li>Finds the most common label within the neighbours.</li>\n",
    "                <li>Returns the most common label as the prediction for the new example.</li>\n",
    "            </ol>\n",
    "        </li>\n",
    "    </ol>\n",
    "    There are lots of different ways to implement this functionality in Python. For this activity it is suggested you use <b>Generative AI</b> tools to complete the <code>predict_one()</code> method (unless you prefer to code it yourself!). <br><br>\n",
    "    The next cell below lets you test your KNN implementation with K=1 on the Iris data. You should find it reaches the same accuracy as the <code>Simple1NNClassifier()</code>. Then in the following activity we will think about how to test your implementation to verify it is correct.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Hints:</b> \n",
    "    <ul>\n",
    "        <li>If you choose to use a GenAI tool with a text/web interface, like ChatGPT or Gemini, you should think carefully about how you construct the prompt. You aren't asking for a full KNN implementation, only a function that makes predictions for 1 new example, so:\n",
    "            <ul>\n",
    "                <li>Specify the parameters the function takes.</li>\n",
    "                <li>What the output should be.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>If you choose to use Github Copilot (or similar), you might find it easier to start typing the function definition (<code>def predict_one()</code>) and see if it provides a completion suggestion.\n",
    "            <ul><li>Or, see if you can get copilot to help you complete each step above individually.</li></ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleKNNClassifier(Simple1NNClassifier):\n",
    "    \"\"\"Complete this class to produce a KNN classifier\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor for the KNN classifier\n",
    "        you will need to change the function signature to expect and store a parameter K\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Complete the function\")\n",
    "\n",
    "    def predict_one(self, item_idx:int, distances:np.ndarray):\n",
    "        \"\"\" makes a class prediction for a single new item\n",
    "        This version is for K Nearest Neighbour\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        item_idx: int\n",
    "            index of item to make prediction for - i.e. idx of row in distances matrix\n",
    "\n",
    "        distances: numpy ndarray\n",
    "            array of distances between new items (rows) and training set records (columns)\n",
    "        \"\"\"\n",
    "        # ====> insert your code below here\n",
    "\n",
    "        raise NotImplementedError(\"Complete the function\")\n",
    "        \n",
    "        # <==== insert your code above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with K=1\n",
    "my_KNN_model = SimpleKNNClassifier(K=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "my_KNN_model.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions for test data\n",
    "predictions = my_KNN_model.predict(test_x)\n",
    "print(f'Predictions are:\\n {predictions}')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"\\nOverall Accuracy = {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 2: Evaluating KNN and GenAI</h2>\n",
    "    Generative AI is very powerful and can save us a lot of time by helping to write code. However, you <b>should not blindly trust any code (or any thing) GenAI creates</b>. GenAI doesn't really <i>'understand'</i> what we are asking it to do. It is simply generating text/code/images based on its <i>prediction</i> of what is most likely correct. This does not mean it is correct, and sometimes it will <i>'hallucinate'</i>, i.e. make stuff up!<br>\n",
    "    So you should always <i>verify</i> anything you ask GenAI to create. For code this should come from your understanding of the code it has generated and the algorithm. But for a more concrete test we can also write some test cases to check the output is correct.<br><br>\n",
    "    Your <code>SimpleKNNClassifier()</code> with <code>K=1</code> should reach the same accuracy as the previous <code>Simple1NNClassifier()</code> did. But we should check it works for other values of K and also on other data.<br>\n",
    "    Complete the following 3 cells to implement 3 different test cases and evaluate your GenAI version of KNN:\n",
    "    <ol>\n",
    "        <li>Compare with the Sklearn implementation of KNN on Iris data. In this case, we know the Sklearn KNN classifier is correct. So we can compare the Sklearn KNN predictions with our own implementation:\n",
    "            <ul>\n",
    "                <li>Create an instance of <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">Sklearn kNeighboursClassifier</a> and run it on the Iris data to make predictions.</li>\n",
    "                <li>Create an instance of your <code>SimpleKNNClassifier()</code> and run it on the Iris data to make predictions.</li>\n",
    "                <li>Compare Sklearn and your classifier predictions accuracy on different values of K.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Compare with the Sklearn implementation of KNN on random data. This is the same as step 1, except instead of Iris data we will randomly generate some.\n",
    "            <ul>\n",
    "                <li>Use the <a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.make_multilabel_classification.html#sklearn.datasets.make_multilabel_classification\">Sklearn make_multilabel_classification()</a> function to create a dataset with  500 samples, 2 features and 4 classes. Or experiment with different numbers of features/classes.</li>\n",
    "                <li>Compare Sklearn and your classifier as before.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Construct simple 2D data to test KNN. We can manually construct data that will give different predictions for different values of K, e.g. K=1 predict 0, K=3 predict 1, K=5 predict 0 etc.\n",
    "            <ul>\n",
    "                <li>Create two 2D arrays of 5 examples (data points) each. One array for class 0 and one array for class 1.</li>\n",
    "                <li>Test on a single example. Suggest you use <code>[4.1, 4.1]</code> but you can use another if you prefer!</li>\n",
    "                <li>Compare Sklearn and your classifier as before. The predictions should be different for increasing values of K, e.g. <code>[1, 0, 1, 0, 1]</code> for k values <code>[1, 3, 5, 7, 9]</code> </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1: Compare with the Sklearn implementation of KNN on Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Test different values of K\n",
    "for K in [1, 3, 5, 7, 9]:\n",
    "    # ====> insert your code below here\n",
    "\n",
    "\n",
    "    # <==== insert your code above here\n",
    "    print(f\"Accuracy of Sklearn model is {skl_acc:.2f}%, your model is {my_acc:.2f}%\")\n",
    "    assert skl_acc == my_acc, f\"Accuracy of Sklearn model is not the same asyour model for K={K}\"\n",
    "\n",
    "    # Compare individual predictions\n",
    "    assert (skl_knn_pred == my_knn_pred).all(), \"Predictions are not the same for K={K}\"\n",
    "    print(f\"Sklearn and your model make the same predictions for K={K}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2: Compare with the Sklearn implementation of KNN on random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a random dataset with 500 samples, 2 features and 4 classes\n",
    "# ====> insert your code below here\n",
    "\n",
    "# <==== insert your code above here\n",
    "\n",
    "# Visualise the data\n",
    "fig, an = plt.subplots(figsize=(8, 8))\n",
    "an.scatter(X[:, 0], X[:, 1], c=y)\n",
    "an.set_title(\"Random dataset with 4 classes\")\n",
    "plt.show()\n",
    "\n",
    "# Create a train/test split of the data\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "\n",
    "# Test different values of K\n",
    "for K in [1, 3, 5, 7, 9]:\n",
    "    # ====> insert your code below here\n",
    "\n",
    "\n",
    "    # <==== insert your code above here\n",
    "    print(f\"Accuracy of Sklearn model is {skl_acc:.2f}%, your model is {my_acc:.2f}%\")\n",
    "    assert skl_acc == my_acc, f\"Accuracy of Sklearn model is not the same asyour model for K={K}\"\n",
    "\n",
    "    # Compare individual predictions\n",
    "    assert (skl_knn_pred == my_knn_pred).all(), \"Predictions are not the same for K={K}\"\n",
    "    print(f\"Sklearn and your model make the same predictions for K={K}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 3: Construct simple 2D data to test KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Generate synthetic 2D data with 5 examples in each class\n",
    "# ====> insert your code below here\n",
    "\n",
    "# <==== insert your code above here\n",
    "\n",
    "# Combine data and labels\n",
    "X = np.vstack((class_0, class_1))\n",
    "y = [0] * len(class_0) + [1] * len(class_1)\n",
    "\n",
    "# Test point\n",
    "test_y = np.array([[4.1, 4.1]])\n",
    "\n",
    "# Visualise the data\n",
    "fig, an = plt.subplots(figsize=(8, 8))\n",
    "an.scatter(X[:, 0], X[:, 1], c=y)\n",
    "an.scatter(test_y[:, 0], test_y[:, 1], c='red', marker='x', s=100)\n",
    "an.set_title(\"Synthetic dataset with 2 classes\")\n",
    "plt.show()\n",
    "\n",
    "# Test different values of K\n",
    "predictions = []\n",
    "for K in [1, 3, 5, 7, 9]:\n",
    "    # ====> insert your code below here\n",
    "\n",
    "\n",
    "    # <==== insert your code above here\n",
    "    assert skl_knn_pred == my_knn_pred, f\"Prediction is not the same for K={K}\"\n",
    "    predictions.append(my_knn_pred[0].item())\n",
    "    print(f\"Sklearn and your model make the same predictions {my_knn_pred[0]} and {skl_knn_pred[0]} for K={K}\\n\")\n",
    "\n",
    "# Check the predictions were different for different values of K\n",
    "assert predictions == [1, 0, 1, 0, 1], \"Predictions are not as expected\"\n",
    "print(f\"All predictions are as expected {predictions} for different values of K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 3: Experiment with KNN</h2>\n",
    "Now that you have established that your implementation of KNN is correct, lets explore a few things that might effect its output predictions.\n",
    "\n",
    "Creating different splits of the Iris data using  Sklearn's <code>train_test_split()</code> function:\n",
    "    <ul>\n",
    "        <li>Try changing the <code>random_state</code> parameter.</li>\n",
    "        <li>Try removing the <code>stratify</code> parameter.</li>\n",
    "        <li>Try changing the <code>test_size</code> parameter.</li>\n",
    "        <li>Does it reach the same accuracy/predictions as before?</li>\n",
    "        <li>If these are not the same, can you explain why not?</li>\n",
    "    </ul>\n",
    "\n",
    "In Machine Learning we talk about algorithms having <b>hyper-parameters</b> that control their behaviour. For KNN <code>K</code> is a <i>hyperparamter</i>. Try running and evaluating KNN with K = {3, 5, 7, 9}:\n",
    "    <ul>\n",
    "        <li>Make <b>qualitative</b> judgements: how does the decision surface change? (you might need to select 2 features as before).</li>\n",
    "        <li>Make <b>quantitative</b> judgements:  how does the confusion matrix change?</li>\n",
    "        <li>What value for the hyper-parameter <b>K</b> gives the best accuracy on the <b>train</b> set?</li>\n",
    "        <li>What value for the hyper-parameter <b>K</b> gives the best accuracy on the <b>test</b> set?</li>\n",
    "        <li>If these are not the same, can you explain why not?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make train/test split of datasets\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy, random_state=42)\n",
    "print(f'Training set has {train_x.shape[0]} examples, test set has {test_x.shape[0]} examples')\n",
    "\n",
    "# Set K\n",
    "K = 3\n",
    "\n",
    "# Instantiate and fit model to data\n",
    "knn = SimpleKNNClassifier(K=K)\n",
    "knn.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = knn.predict(test_x)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"Overall Accuracy for K={K} is {accuracy:.2f}%\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_predictions(test_y, predictions, display_labels=label_names)\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.plot()\n",
    "\n",
    "# Create the decision boundary learned by model from training data\n",
    "trainx_2_features = train_x[:, :2]\n",
    "testx_2_features = test_x[:, :2]\n",
    "knn_2d = KNeighborsClassifier(n_neighbors=K).fit(trainx_2_features, train_y)\n",
    "disp = DecisionBoundaryDisplay.from_estimator(knn_2d, testx_2_features, alpha=0.5)\n",
    "disp.ax_.scatter(petal_testx[:, 0], petal_testx[:, 1], c=test_y, edgecolor='black', label='test')\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:black;width:100%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Decision Trees\n",
    "\n",
    "In the lecture notebook we illustrated how the decision tree is created by dividing input space using a set of axis-parallel lines.\n",
    "\n",
    "The tree is 'grown' by:\n",
    "1. Start with single node that predicts majority class label.\n",
    "2. Loop over every leaf node:\n",
    "    - Measure (in some way) the \"information content\" of the data that arrives at that node.\n",
    "    - For each possible data split:\n",
    "        - measure and add the \"information content\" of the child nodes created by the split\n",
    "        - subtract information content of parent\n",
    "        - result is the *gain* in information content given by split\n",
    "        - update stored \"best split\" if appropriate\n",
    "    - If the  \"best\" split is above some threshold then change the leaf node to an interior node with the *best* condition.\n",
    "    - If <i>termination criteria</i> not met goto step 2.\n",
    "\n",
    "The following cells demonstrate how to create, train and evaluate a Decision Tree using Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train/test split of datasets\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy)\n",
    "print(f'Training set has {train_x.shape[0]} examples, test set has {test_x.shape[0]} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the model class then *fit*  it to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "# Instantiate and fit model to data\n",
    "my_dt = DecisionTreeClassifier()\n",
    "my_dt.fit(train_x, train_y)\n",
    "\n",
    "# Show the decision tree\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "tree.plot_tree(my_dt, feature_names=feature_names, class_names=label_names, filled=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model's performance on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for test data\n",
    "predictions = my_dt.predict(test_x)\n",
    "print(f'Predictions are:\\n {predictions}')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"\\nOverall Accuracy = {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_predictions(test_y, predictions, display_labels=label_names)\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Make data using numpy slicing to just pull the last two columns for every row\n",
    "petal_trainx = train_x[:, :2]\n",
    "petal_testx = test_x[:, :2]\n",
    "\n",
    "# Instantiate and fit model to data\n",
    "my_dt_2d = DecisionTreeClassifier()\n",
    "my_dt_2d.fit(petal_trainx, train_y)\n",
    "\n",
    "# Make predictions, score them \n",
    "y_pred = my_dt_2d.predict(petal_testx)\n",
    "accuracy = 100 * ( test_y == y_pred).sum() / test_y.shape[0]\n",
    "num_errors= len(y_pred) - (test_y == y_pred).sum()\n",
    "print(f\"Overall Accuracy in 2D = {accuracy}%, model makes {num_errors} mistakes\")\n",
    "\n",
    "# Create the decision boundary learned by model from training data\n",
    "disp = DecisionBoundaryDisplay.from_estimator(my_dt_2d, petal_trainx, alpha=0.5)\n",
    "\n",
    "# # Show where the test is within the decision boundary\n",
    "disp.ax_.scatter(petal_testx[:, 0], petal_testx[:, 1], c=test_y, edgecolor='black', label='test')\n",
    "_= disp.ax_.set_title(\"Decision Tree decision boundary on petal features\")\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 4: Experiment with Decision Trees</h2>\n",
    "We often want to control how we learn a model (in this case, grow a tree) to avoid a phenomenon call <i>over-fitting</i>. This is where the model is capturing fine-details of the training set and so failing to generalise from the training set to the real world.\n",
    "\n",
    "The aim of this activity is for you to experiment with what happens when you change two <b>hyper-parameters</b> that affect how big and complex the tree is allowed to get.\n",
    "<ul>\n",
    "    <li><code>max_depth</code>: default is None</li>\n",
    "    <li><code>min_samples_leaf</code>: default value is 1</li>\n",
    "</ul>\n",
    "Experiment with the Iris data set we loaded earlier to see if you can work out what each of these hyper-parameters does, and how it affects the tree. Try running and evaluating KNN with different values for <code>max_depth</code> = {None, 1, 3, 5} and <code>min_samples_leaf</code> = {1, 3, 5}:\n",
    "    <ul>\n",
    "        <li>Do some combinations result in bigger differences between accuracy on the train / test sets?</li>\n",
    "        <li>Is there a combination of hyper-parameter values that means you consistently get similar trees?</li>\n",
    "        <li>What is a good way of judging 'similarity?</li>\n",
    "        <li>Do different train/test splits affect what tree you get?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make train/test split of datasets\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy, random_state=42)\n",
    "print(f'Training set has {train_x.shape[0]} examples, test set has {test_x.shape[0]} examples')\n",
    "\n",
    "# Experiment with changing these values\n",
    "depth = None  # Try None 1, 3, 5\n",
    "min_leaf = 3  # Try 1, 3, 5\n",
    "\n",
    "# Instantiate and fit model to data\n",
    "my_dt = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_leaf)\n",
    "my_dt.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = my_dt.predict(test_x)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"Overall Accuracy for depth={depth} and min_leaf={min_leaf} is {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Plot the confusion matrix and the tree side by side\n",
    "fig, ax= plt.subplots(ncols=2, figsize=(8, 8)) \n",
    "cm = ConfusionMatrixDisplay.from_predictions(test_y, predictions, display_labels=label_names, colorbar=False, ax=ax[0])\n",
    "_ = tree.plot_tree(my_dt, feature_names=feature_names, class_names=label_names, filled=True, ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Save and close Jupyter:</b>\n",
    "    <ol>\n",
    "        <li>Use the jupyterlab functions to download your work (ask your tutor if you need help with this) and save it somewhere sensible so you can find it easily.</li>\n",
    "        <li>Shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook)</li>\n",
    "    </ol>\n",
    "</div"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
